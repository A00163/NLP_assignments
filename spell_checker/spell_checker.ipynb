{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download necessary NLTK data for tokenization\n"
      ],
      "metadata": {
        "id": "FFF8C8MASNpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWMSdjmF2DX8",
        "outputId": "d0a716de-6759-4db1-90cf-bdb23dbb6723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import string"
      ],
      "metadata": {
        "id": "JDtEf3j0O9vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the vocabulary file and store words in a set for fast lookup\n"
      ],
      "metadata": {
        "id": "nDFxTjndSVUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/Vocabulary.txt', 'r', encoding='utf-8') as f:\n",
        "        vocabulary = set(word.strip() for word in f.readlines())"
      ],
      "metadata": {
        "id": "z0Q101g-O-z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to calculate the Damerau-Levenshtein distance between two strings\n"
      ],
      "metadata": {
        "id": "RfZURkKlSY19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def damerau_levenshtein_distance(s1, s2):\n",
        "    dp = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]\n",
        "\n",
        "    for i in range(len(s1) + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(len(s2) + 1):\n",
        "        dp[0][j] = j\n",
        "\n",
        "    for i in range(1, len(s1) + 1):\n",
        "        for j in range(1, len(s2) + 1):\n",
        "            if s1[i - 1] == s2[j - 1]:\n",
        "                cost = 0\n",
        "            else:\n",
        "                cost = 1\n",
        "\n",
        "            dp[i][j] = min(dp[i - 1][j] + 1,\n",
        "                           dp[i][j - 1] + 1,\n",
        "                           dp[i - 1][j - 1] + cost)\n",
        "\n",
        "            if i > 1 and j > 1 and s1[i - 1] == s2[j - 2] and s1[i - 2] == s2[j - 1]:\n",
        "                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + cost)\n",
        "\n",
        "    return dp[len(s1)][len(s2)]"
      ],
      "metadata": {
        "id": "3MOqPvazPB4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to suggest corrections for a given word\n"
      ],
      "metadata": {
        "id": "nQpgYp3pSdqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_corrections(word, vocabulary, split_hyphen=True):\n",
        "    if split_hyphen and '-' in word:\n",
        "        parts = word.split('-')\n",
        "        suggestions = [suggest_corrections(part, vocabulary, split_hyphen=False) for part in parts]\n",
        "        return '-'.join(suggestions)\n",
        "    else:\n",
        "        min_distance = float('inf')\n",
        "        best_correction = None\n",
        "        for correct_word in vocabulary:\n",
        "            distance = damerau_levenshtein_distance(word, correct_word)\n",
        "            if distance < min_distance:\n",
        "                min_distance = distance\n",
        "                best_correction = correct_word\n",
        "        return best_correction"
      ],
      "metadata": {
        "id": "y6ljqNPLPE6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the input sentence to be processed\n"
      ],
      "metadata": {
        "id": "mh8kNmauSk6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Multipmodal usion si a ore proble fofr mpltimodal seuntiment aanlysis.\"\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "# Process each word in the sentence\n",
        "for word in words:\n",
        "  if word.lower() not in string.punctuation:\n",
        "    if word.lower() not in vocabulary:\n",
        "        correction = suggest_corrections(word.lower(), vocabulary)\n",
        "        print(f\"Suggestion for '{word}': {correction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LuSMizmPIKi",
        "outputId": "cbd22dbc-8402-4aa4-d00c-7482fce8d7dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suggestion for 'Multipmodal': multinomial\n",
            "Suggestion for 'usion': union\n",
            "Suggestion for 'si': bi\n",
            "Suggestion for 'proble': problem\n",
            "Suggestion for 'fofr': four\n",
            "Suggestion for 'mpltimodal': optional\n",
            "Suggestion for 'seuntiment': sentiment\n",
            "Suggestion for 'aanlysis': analysis\n"
          ]
        }
      ]
    }
  ]
}