### Ú©Ø¯ Ø¨Ø§ Ú©Ø§Ù…Ù†Øªâ€ŒÙ‡Ø§

Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ú©Ø¯ Ø¢Ù¾Ù„ÙˆØ¯Ø´Ø¯Ù‡ Ø¨Ø§ ØªÙˆØ¶ÛŒØ­Ø§Øª Ùˆ Ú©Ø§Ù…Ù†Øªâ€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¨Ø®Ø´ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª:

```python
import nltk
# Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ
nltk.download('punkt')

from nltk.tokenize import word_tokenize
import string

# Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Vocabulary.txt Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§Øª Ø¯Ø± ÛŒÚ© Ù…Ø¬Ù…ÙˆØ¹Ù‡ (Set)
with open('/content/Vocabulary.txt', 'r', encoding='utf-8') as f:
    vocabulary = set(word.strip() for word in f.readlines())

# ØªØ§Ø¨Ø¹ Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ§ØµÙ„Ù‡ Damerau-Levenshtein Ø¨Ø±Ø§ÛŒ ØªØµØ­ÛŒØ­ Ú©Ù„Ù…Ø§Øª
def damerau_levenshtein_distance(s1, s2):
    # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø§ØªØ±ÛŒØ³ DP Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ù…Ø­Ø§Ø³Ø¨Ø§Øª
    dp = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]

    # Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù…Ø§ØªØ±ÛŒØ³
    for i in range(len(s1) + 1):
        dp[i][0] = i
    for j in range(len(s2) + 1):
        dp[0][j] = j

    # Ù¾Ø± Ú©Ø±Ø¯Ù† Ù…Ø§ØªØ±ÛŒØ³ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¹Ù…Ù„ÛŒØ§Øª Ø¯Ø±Ø¬ØŒ Ø­Ø°ÙØŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ùˆ Ø¬Ø§Ø¨Ø¬Ø§ÛŒÛŒ
    for i in range(1, len(s1) + 1):
        for j in range(1, len(s2) + 1):
            if s1[i - 1] == s2[j - 1]:
                cost = 0
            else:
                cost = 1

            dp[i][j] = min(dp[i - 1][j] + 1,  # Ø­Ø°Ù
                           dp[i][j - 1] + 1,  # Ø¯Ø±Ø¬
                           dp[i - 1][j - 1] + cost)  # Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ

            # Ø¨Ø±Ø±Ø³ÛŒ Ø¬Ø§Ø¨Ø¬Ø§ÛŒÛŒ Ø­Ø±ÙˆÙ (transposition)
            if i > 1 and j > 1 and s1[i - 1] == s2[j - 2] and s1[i - 2] == s2[j - 1]:
                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + cost)

    return dp[len(s1)][len(s2)]

# ØªØ§Ø¨Ø¹ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ ØªØµØ­ÛŒØ­Ø§Øª Ø¨Ø±Ø§ÛŒ Ú©Ù„Ù…Ø§Øª
def suggest_corrections(word, vocabulary, split_hyphen=True):
    # Ø§Ú¯Ø± Ú©Ù„Ù…Ù‡ Ø´Ø§Ù…Ù„ Ø®Ø· ØªÛŒØ±Ù‡ Ø¨Ø§Ø´Ø¯ØŒ Ø¢Ù† Ø±Ø§ Ø¨Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
    if split_hyphen and '-' in word:
        parts = word.split('-')
        suggestions = [suggest_corrections(part, vocabulary, split_hyphen=False) for part in parts]
        return '-'.join(suggestions)
    else:
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ù‡ ØµØ­ÛŒØ­ Ø¨Ø§ Ú©Ù…ØªØ±ÛŒÙ† ÙØ§ØµÙ„Ù‡
        min_distance = float('inf')
        best_correction = None
        for correct_word in vocabulary:
            distance = damerau_levenshtein_distance(word, correct_word)
            if distance < min_distance:
                min_distance = distance
                best_correction = correct_word
        return best_correction

# Ø¬Ù…Ù„Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ù‚Ø±Ø§Ø± Ø§Ø³Øª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´ÙˆØ¯
sentence = "Multipmodal usion si a ore proble fofr mpltimodal seuntiment aanlysis."
# ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ø¬Ù…Ù„Ù‡ Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª
words = word_tokenize(sentence)

# Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ø± Ú©Ù„Ù…Ù‡ Ø¯Ø± Ø¬Ù…Ù„Ù‡
for word in words:
    # Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ† Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ
    if word.lower() not in string.punctuation:
        # Ø§Ú¯Ø± Ú©Ù„Ù…Ù‡ Ø¯Ø± ÙˆØ§Ú˜Ú¯Ø§Ù† Ù…ÙˆØ¬ÙˆØ¯ Ù†Ø¨Ø§Ø´Ø¯ØŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ ØªØµØ­ÛŒØ­ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        if word.lower() not in vocabulary:
            correction = suggest_corrections(word.lower(), vocabulary)
            print(f"Suggestion for '{word}': {correction}")
```

---

### ÙØ§ÛŒÙ„ README

Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ØŒ ÙØ§ÛŒÙ„ README Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ú©Ø¯ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª:

---

# Spell Checker with Damerau-Levenshtein Distance

This Python script is designed to process a sentence, identify misspelled words, and suggest corrections based on a predefined vocabulary file. It uses the Damerau-Levenshtein distance algorithm to find the closest match for each word.

---

## Features

- **Custom Vocabulary**: Reads a list of correct words from a file (`Vocabulary.txt`).
- **Error Detection**: Identifies words in a sentence that are not in the vocabulary.
- **Error Correction**: Suggests corrections for misspelled words using Damerau-Levenshtein distance.
- **Handles Hyphenated Words**: Supports splitting and correcting hyphenated words.

---

## Requirements

### Python Version
- Python 3.6 or higher

### Required Libraries
Before running the script, ensure the following libraries are installed:
- `nltk`

You can install them using pip:
```bash
pip install nltk
```

---

## Usage

### 1. Prepare the Vocabulary File
Create a file named `Vocabulary.txt` and add a list of valid words, one word per line. For example:
```
multimodal
fusion
is
a
core
problem
for
sentiment
analysis
```

### 2. Write Your Sentence
Modify the `sentence` variable in the script to include the text you want to process. For example:
```python
sentence = "Multipmodal usion si a ore proble fofr mpltimodal seuntiment aanlysis."
```

### 3. Run the Script
Run the script using Python:
```bash
python nlp_ex1_3.py
```

### 4. Output
The script will print suggestions for misspelled words. For example:
```
Suggestion for 'Multipmodal': multimodal
Suggestion for 'usion': fusion
Suggestion for 'si': is
Suggestion for 'ore': core
Suggestion for 'proble': problem
Suggestion for 'fofr': for
Suggestion for 'mpltimodal': multimodal
Suggestion for 'seuntiment': sentiment
Suggestion for 'aanlysis': analysis
```

---

## Code Structure

### Main Steps:
1. **Vocabulary Loading**:
   - Reads the vocabulary from `Vocabulary.txt` and stores it in a set for fast lookup.
2. **Sentence Tokenization**:
   - Tokenizes the input sentence into individual words using `nltk.word_tokenize`.
3. **Error Detection and Correction**:
   - For each word not found in the vocabulary, calculates the Damerau-Levenshtein distance to suggest the closest match.
4. **Handles Hyphenated Words**:
   - Splits words containing hyphens and processes each part separately.




If you have any questions or encounter issues, feel free to ask for help. ğŸ˜Š